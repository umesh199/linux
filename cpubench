#!/usr/bin/env python3

import csv
import subprocess
import os
import re

INVENTORY_FILE = "inventory"
OUTPUT_CSV = "sysbench_cpu_results.csv"
ANSIBLE_PLAYBOOK = "sysbench_cpu_playbook.yml"

NUM_THREADS = 32
PRIME_NUMBER_LIMIT = 10000

def generate_ansible_playbook():
    """Creates an Ansible playbook for running sysbench CPU benchmark."""
    playbook_content = f"""
    - hosts: all
      gather_facts: yes
      tasks:
        - name: Install sysbench
          package:
            name: sysbench
            state: present

        - name: Run CPU benchmark
          command: sysbench cpu --threads={NUM_THREADS} --cpu-max-prime={PRIME_NUMBER_LIMIT} run
          register: cpu_result

        - name: Save CPU benchmark result
          copy:
            content: "{{{{ cpu_result.stdout }}}}"
            dest: "/tmp/sysbench_cpu_results_{{{{ inventory_hostname }}}}.txt"
    """
    
    with open(ANSIBLE_PLAYBOOK, "w") as file:
        file.write(playbook_content)

def run_ansible_playbook():
    """Runs the Ansible playbook to execute CPU benchmark tests on remote hosts."""
    subprocess.run(["ansible-playbook", "-i", INVENTORY_FILE, ANSIBLE_PLAYBOOK], check=True)

def fetch_results():
    """Fetches results from remote hosts using Ansible fetch module."""
    os.makedirs("results", exist_ok=True)
    
    fetch_command = [
        "ansible", "all", "-i", INVENTORY_FILE, "-m", "fetch", "-a",
        "src=/tmp/sysbench_cpu_results_{{ inventory_hostname }}.txt dest=results/ flat=yes"
    ]
    
    subprocess.run(fetch_command, check=True)

def parse_sysbench_output(output):
    """Extracts performance metrics from sysbench output."""
    metrics = {
        "Events Per Second": "N/A",
        "Total Time (s)": "N/A",
        "Total Number of Events": "N/A",
        "Latency (min)": "N/A",
        "Latency (avg)": "N/A",
        "Latency (max)": "N/A",
        "Latency (95th percentile)": "N/A"
    }

    # Regular expressions for extracting relevant details
    patterns = {
        "Events Per Second": r"events per second:\s+([\d.]+)",
        "Total Time (s)": r"total time:\s+([\d.]+)s",
        "Total Number of Events": r"total number of events:\s+(\d+)",
        "Latency (min)": r"min:\s+([\d.]+)",
        "Latency (avg)": r"avg:\s+([\d.]+)",
        "Latency (max)": r"max:\s+([\d.]+)",
        "Latency (95th percentile)": r"95th percentile:\s+([\d.]+)"
    }

    for key, pattern in patterns.items():
        match = re.search(pattern, output)
        if match:
            metrics[key] = match.group(1)

    return metrics

def parse_results():
    """Parses fetched results and writes them to a CSV file."""
    results = []

    for filename in os.listdir("results"):
        hostname = filename.replace("sysbench_cpu_results_", "").replace(".txt", "")
        filepath = os.path.join("results", filename)

        with open(filepath, "r") as file:
            data = file.read()
            metrics = parse_sysbench_output(data)
            results.append([hostname, NUM_THREADS, PRIME_NUMBER_LIMIT, 
                            metrics["Events Per Second"], metrics["Total Time (s)"],
                            metrics["Total Number of Events"], 
                            metrics["Latency (min)"], metrics["Latency (avg)"], 
                            metrics["Latency (max)"], metrics["Latency (95th percentile)"]])

    with open(OUTPUT_CSV, mode="w", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(["Hostname", "Number of Threads", "Prime Number Limit",
                         "Events Per Second", "Total Time (s)", 
                         "Total Number of Events", "Latency (min)", 
                         "Latency (avg)", "Latency (max)", "Latency (95th percentile)"])
        writer.writerows(results)

def main():
    generate_ansible_playbook()
    run_ansible_playbook()
    fetch_results()
    parse_results()
    print(f"âœ… CPU benchmarking completed. Results saved in {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
